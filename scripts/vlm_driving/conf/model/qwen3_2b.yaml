# Qwen3-VL-2B model configuration
model_id: "Qwen/Qwen2-VL-2B-Instruct"  # Update when Qwen3-VL is released
processor_id: ${model_id}

# Quantization configuration (QLoRA)
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_use_double_quant: true

# Attention implementation
attn_implementation: "flash_attention_2"

# Model loading
device_map: "auto"
torch_dtype: "float16"

# Generation settings (for inference)
generation:
  max_new_tokens: 1
  do_sample: false
  temperature: 1.0
  top_p: 1.0

# LoRA configuration
lora:
  r: 64
  lora_alpha: 128
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
  vision_lora: true
