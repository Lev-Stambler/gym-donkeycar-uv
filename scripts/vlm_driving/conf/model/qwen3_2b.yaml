# Qwen3-VL-2B model configuration
model_id: "Qwen/Qwen3-VL-2B-Instruct"
processor_id: "Qwen/Qwen3-VL-2B-Instruct"

# Quantization configuration (QLoRA)
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"
  bnb_4bit_use_double_quant: true

# Attention implementation (use "sdpa" if flash_attn not installed)
attn_implementation: "sdpa"

# Model loading
device_map: "auto"
torch_dtype: "float16"

# Generation settings (for inference)
generation:
  max_new_tokens: 1
  do_sample: false
  temperature: 1.0
  top_p: 1.0

# LoRA configuration
lora:
  r: 64
  lora_alpha: 128
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"
  vision_lora: true
